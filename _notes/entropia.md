---
---

- $H(X) = -\sum_{i=1}^{n} p_i \log_2 p_i$
- Entropy is like the messy state of your closet - it's the measure of disorder or randomness in a system. Em teoria da informação, a entropia é usada para medir a quantidade de informação contida em uma mensagem. Por exemplo, se eu mandar uma mensagem dizendo "oi", ela terá uma quantidade de informação menor do que se eu mandasse uma mensagem grande com várias palavras que você nunca viu de mim antes. Isso porque a primeira mensagem é mais previsível, enquanto a segunda tem mais incerteza.
- Na teoria da informação, entropia é usada para medir a quantidade de informação em um conjunto de dados. Por exemplo, imagine que você tem uma caixa com um monte de bolinhas de diferentes cores. Se todas as bolinhas forem da mesma cor, a caixa estará em um estado de baixa entropia, já que é previsível o que vai acontecer ao retirarmos uma bolinha. Agora, se as bolinhas forem de várias cores diferentes, a caixa estará em um estado de alta entropia, já que não sabemos com certeza qual bolinha vamos tirar.
-